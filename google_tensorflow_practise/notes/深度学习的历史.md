### 深度学习的历史

第一阶段

1. 首次提出仿大脑的学习机理，最开始为n个输入的线性加权+阈值判定
2. 为解决权重的自动设置问题，提出了感知机器，但是感知机模型只能解决线性可分问题，而且基于当时的算力，无法实现深层神经网络。

------------------

第二阶段

1. 分布式知识表达的提出：现实世界中的知识和概念应该通过多个神经元来表达
2. 反向传播算法

但是这一阶段的算力仍然不足，训练样本也不够。传统的机器学习算法也得到了突破性的发展。

-----------

第三阶段

1. 算力得到了极大的提升
2. 训练样本也非常充足
3. ImageNet竞赛的图像分类，引爆了深度学习



### Tensorflow的三大结构

1. 计算模型：Tensorflow中使用计算图表示计算模型，计算图可以管理变量资源、隔离资源等
2. 数据模型：使用张量（类似于numpy中的数组，但是并不是直接存储数据，而只是存一个引用）
3. 会话模型：掌握资源着，执行者。可用于设置计算图的计算方式，配置等



### 深度学习--深层神经网络

深层神经网络强调**非线性**和**深层结构。**

- 深层的线性结构和简单的线性结构没有本质区别，现实生活中大多情况下都是非线性的
- 深层结构能够实现组合特征提取，从输入中抽取出更高维的特征



### 损失函数

对于分类问题，常使用交叉熵。交叉熵代表的是两个概率分布之间的距离，而神经网络输入的并不一定是概率分布，所以可以通过加入softmax层来使输出变成概率分布，再套用交叉熵公式即可。在Tensorflow中，交叉熵和softmax经常在一起使用，所以tensorflow将两者封装起来了，


### 采用batch的意义
使用batch可防止海量数据造成的内存不足，同时也加速了了损失函数收敛的速度

### 深度学习中遇见的常见问题

1. 学习速率的设定问题

   如果将学习速率设置过大，可能导致无法寻找到局部最优解（震荡问题）。如果将学习速率设置过小，会使得优化速度过慢，迭代次数过长。所以可**常用学习速率指数下降法来进行设置**。

2. 防止过拟合问题

   过拟合指的是，神经网络的参数过度拟合训练数据，导致无法反应数据的趋势和规律。**防止过拟合可采用正则化的方式，有正则化1和正则化2**.

3. 使用滑动平均可以使得模型更加健壮